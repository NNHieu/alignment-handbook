import os
from datasets import load_dataset
import argparse
import json
from pathlib import Path
import torch
import pyarrow.parquet as pq
import logging
import os
import random
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import datasets
import pandas as pd
import gc
import os
from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel
os.environ["CUDA_VISIBLE_DEVICES"] = '2,3,4,5'
import argparse

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0')
    parser.add_argument('--revision', type=str, default=None)
    parser.add_argument('--model_alias', type=str, required=True)
    parser.add_argument('--data_frac', type=int, default=0)
    parser.add_argument('--frac_len', type=int, default=0)
    parser.add_argument('--data_path', type=str, required=True)
    parser.add_argument('--data_alias', type=str, required=True)
    # parser.add_argument('--output_dir', type=str, default='generated/iter1')
    # parser.add_argument('--world_size', type=int, default=8) # controls the number of gpus vLLM is allowed to use
    # parser.add_argument('--input_dir', type=str, default='UCLA-AGI/SPIN_iter0')
    # parser.add_argument('--split', type=str, default='train')
    return parser.parse_args()
args = parse_arguments()

model_path = args.model
model_alias = args.model.split('/')[-1]

# data_path = 'UCLA-AGI/SPIN_iter0'
data_path = args.data_path
output_dir = Path('data/spin_analyze')
output_dir = output_dir / args.data_alias / model_alias
output_dir.mkdir(parents=True, exist_ok=True)
# data_path = 'HuggingFaceH4/ultrafeedback_binarized'
# output_dir = Path('data/spin_data/ultrafeedback_binarized') / args.model_alias
data_frac = args.data_frac
# frac_len = 5001
frac_len = args.frac_len


# model_path = "/home/ubuntu/hieu.nn/Lang/alignment-handbook/data/stablelm-2-1_6b-spin-dpo-0-full"

tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

dataset = load_dataset(data_path, split='train')

def apply_chat_template(example, tokenizer, ):
    messages = example['generated']
    if messages[0]["role"] != "system":
        messages.insert(0, {"role": "system", "content": ""})
    example['text_generated'] = tokenizer.apply_chat_template(messages[:3], tokenize=False, add_generation_prompt=False)

    messages = example['real']
    if messages[0]["role"] != "system":
        messages.insert(0, {"role": "system", "content": ""})
    example['text_real'] = tokenizer.apply_chat_template(messages[:3], tokenize=False, add_generation_prompt=False)
    
    return example


# appliedtemplate_datasets = appliedtemplate_datasets.shuffle(42)

if frac_len > 0:
    sub_len = frac_len 
    if sub_len*(data_frac+1) > len(dataset):
        dataset = dataset.select(range(sub_len*data_frac, len(dataset)))
    else:
        dataset = dataset.select(range(sub_len*data_frac,sub_len*(data_frac+1)))
# else:
#     dataset = dataset[:]

appliedtemplate_datasets = dataset.map(
    apply_chat_template,
    fn_kwargs={
        "tokenizer": tokenizer,
        # "task": "sft",
        # "auto_insert_empty_system_msg": True,
    },
    num_proc=12,
    # remove_columns=column_names,
    desc="Applying chat template",
)


llm = LLM(
    model=model_path,
    revision=args.revision,
    tensor_parallel_size=4,
    # gpu_memory_utilization=1.0,
    dtype=torch.bfloat16,
    # max_tokens=1,
    max_num_seqs=1,
)
# sampling_params = SamplingParams(temperature=1.0, top_p=1.0, max_tokens=256)
# results_gathered = list(map(lambda x: x.outputs[0].text, 
#                                   llm.generate(appliedtemplate_datasets['prompt_text'], sampling_params)))
# results = [r.replace("</s>","").lstrip() for r in results_gathered]


all_inputs = []
for example in appliedtemplate_datasets:
    all_inputs.append(example['text_generated'])
    all_inputs.append(example['text_real'])

sampling_params = SamplingParams(temperature=1.0, top_p=1.0, max_tokens=1, logprobs=1, prompt_logprobs=1)
results_gathered = llm.generate(all_inputs, sampling_params)
results = []
prompt_logprobs = []
logprobs = []
cummulative_logprobs = []

for i in range(len(results_gathered)):
    results.append(results_gathered[i].outputs[0].text.replace("</s>","").lstrip())
    if results_gathered[i].prompt_logprobs is not None:
        prompt_logprobs.append([None] + [list(p.values())[0].logprob for p in results_gathered[i].prompt_logprobs[1:]])
    else:
        prompt_logprobs.append([None])
    
    # Ignore the last token logprob as it is generated by the model
    logprobs.append([list(p.values())[0].logprob for p in results_gathered[i].outputs[0].logprobs[:-1]])
    cummulative_logprobs.append(sum(logprobs[-1]))

# Save the results
generated_dataset =[]
for idx in range(0, len(results), 2):
    d = {
            "prompt_id": idx // 2,
            # "prompt": appliedtemplate_datasets[idx // 2]['prompt'],
            "real": appliedtemplate_datasets[idx // 2]['real'],
            "generated": appliedtemplate_datasets[idx // 2]['generated'],
            "generated_logprobs": prompt_logprobs[idx],
            "real_logprobs": prompt_logprobs[idx + 1],
            # "prompt_token_logprobs": prompt_logprobs[idx],
        }
    generated_dataset.append(d)

# appliedtemplate_datasets.add_column("")

ds = datasets.Dataset.from_pandas(pd.DataFrame(data=generated_dataset))
ds.save_to_disk(str(output_dir / f"{data_frac}_{frac_len}"))
# ds = ds.train_test_split(test_size=1)
# ds['test'].to_parquet(output_dir / f"SPIN_iter0/{data_frac}_{frac_len}/test/data.parquet")

# Delete the llm object and free the memory
destroy_model_parallel()
del llm
gc.collect()
torch.cuda.empty_cache()
# torch.distributed.destroy_process_group()
print("Successfully delete the llm pipeline and free the GPU memory!")