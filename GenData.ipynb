{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-24 14:24:12,761\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,4'\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default='UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0')\n",
    "    parser.add_argument('--data_frac', type=int, default=0)\n",
    "    parser.add_argument('--frac_len', type=int, default=0)\n",
    "    # parser.add_argument('--output_dir', type=str, default='generated/iter1')\n",
    "    # parser.add_argument('--world_size', type=int, default=8) # controls the number of gpus vLLM is allowed to use\n",
    "    # parser.add_argument('--input_dir', type=str, default='UCLA-AGI/SPIN_iter0')\n",
    "    # parser.add_argument('--split', type=str, default='train')\n",
    "    return parser.parse_args(\"--data_frac 3 --frac_len 51 --model /home/ubuntu/hieu.nn/Lang/alignment-handbook/data/stablelm-2-1_6b-spin-dpo-2-full\".split())\n",
    "args = parse_arguments()\n",
    "\n",
    "data_path = 'HuggingFaceH4/ultrachat_200k'\n",
    "output_dir = Path('data/spin_data')\n",
    "data_frac = args.data_frac\n",
    "# frac_len = 5001\n",
    "frac_len = args.frac_len\n",
    "\n",
    "\n",
    "# model_path = \"/home/ubuntu/hieu.nn/Lang/alignment-handbook/data/stablelm-2-1_6b-spin-dpo-0-full\"\n",
    "model_path = args.model\n",
    "model_alias = args.model.split('/')[-1]\n",
    "\n",
    "output_dir = output_dir / model_alias\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(data_path, split='train_sft')\n",
    "\n",
    "def apply_chat_template(example, tokenizer, ):\n",
    "    messages = example['messages']\n",
    "    if example[\"messages\"][0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    prompt = tokenizer.apply_chat_template(messages[:2], tokenize=False, add_generation_prompt=True,)\n",
    "    example['prompt_text'] = prompt\n",
    "    return example\n",
    "\n",
    "appliedtemplate_datasets = dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        # \"task\": \"sft\",\n",
    "        # \"auto_insert_empty_system_msg\": True,\n",
    "    },\n",
    "    num_proc=12,\n",
    "    # remove_columns=column_names,\n",
    "    desc=\"Applying chat template\",\n",
    ")\n",
    "\n",
    "appliedtemplate_datasets = appliedtemplate_datasets.shuffle(42)\n",
    "\n",
    "if frac_len > 0:\n",
    "    sub_len = frac_len \n",
    "    if sub_len*(data_frac+1) > len(appliedtemplate_datasets):\n",
    "        appliedtemplate_datasets = appliedtemplate_datasets.select(range(sub_len*data_frac, len(appliedtemplate_datasets)))\n",
    "    else:\n",
    "        appliedtemplate_datasets = appliedtemplate_datasets.select(range(sub_len*data_frac,sub_len*(data_frac+1)))\n",
    "# else:\n",
    "#     appliedtemplate_datasets = appliedtemplate_datasets[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 14:24:29,902\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 14:24:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/home/ubuntu/hieu.nn/Lang/alignment-handbook/data/stablelm-2-1_6b-spin-dpo-2-full', tokenizer='/home/ubuntu/hieu.nn/Lang/alignment-handbook/data/stablelm-2-1_6b-spin-dpo-2-full', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-24 14:24:45 selector.py:16] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:24:46 selector.py:16] Using FlashAttention backend.\n",
      "INFO 04-24 14:24:47 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:24:47 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=253311)\u001b[0m INFO 04-24 14:24:51 model_runner.py:104] Loading model weights took 0.8013 GB\n",
      "INFO 04-24 14:24:51 model_runner.py:104] Loading model weights took 0.8013 GB\n",
      "INFO 04-24 14:24:53 ray_gpu_executor.py:240] # GPU blocks: 13551, # CPU blocks: 5461\n",
      "INFO 04-24 14:24:56 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-24 14:24:56 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:24:56 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:24:56 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=253397)\u001b[0m INFO 04-24 14:24:46 selector.py:16] Using FlashAttention backend.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=253397)\u001b[0m INFO 04-24 14:24:47 pynccl_utils.py:45] vLLM is using nccl==2.18.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:24:51 model_runner.py:104] Loading model weights took 0.8013 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 04-24 14:25:04 model_runner.py:867] Graph capturing finished in 7 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=253156)\u001b[0m INFO 04-24 14:25:04 model_runner.py:867] Graph capturing finished in 7 secs."
     ]
    }
   ],
   "source": [
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=4,\n",
    "    gpu_memory_utilization=0.5, \n",
    ")\n",
    "\n",
    "# results_gathered = list(map(lambda x: x.outputs[0].text, \n",
    "                            # ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 51/51 [00:08<00:00,  6.23it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=1.0, top_p=1.0, max_tokens=256, logprobs=1, prompt_logprobs=1)\n",
    "results_gathered = llm.generate(appliedtemplate_datasets['prompt_text'], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [r.replace(\"</s>\",\"\").lstrip() for r in results_gathered]\n",
    "results = []\n",
    "prompt_logprobs = []\n",
    "logprobs = []\n",
    "cummulative_logprobs = []\n",
    "\n",
    "for i in range(len(results_gathered)):\n",
    "    results.append(results_gathered[i].outputs[0].text.replace(\"</s>\",\"\").lstrip())\n",
    "    prompt_logprobs.append([None] + [list(p.values())[0].logprob for p in results_gathered[i].prompt_logprobs[1:]])\n",
    "    logprobs.append([list(p.values())[0].logprob for p in results_gathered[i].outputs[0].logprobs])\n",
    "    cummulative_logprobs = logprobs[-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_dataset =[]\n",
    "for idx in range(len(results)):\n",
    "    d = {\n",
    "        \"prompt\": appliedtemplate_datasets[idx]['prompt'],\n",
    "        \"prompt_id\": appliedtemplate_datasets[idx]['prompt_id'],\n",
    "        \"chosen\": appliedtemplate_datasets[idx]['messages'][1:3],\n",
    "        \"rejected\": [appliedtemplate_datasets[idx]['messages'][1],\n",
    "                     {\"role\": \"assistant\", \"content\": results[idx]}],\n",
    "        }\n",
    "    generated_dataset.append(d)\n",
    "\n",
    "ds = datasets.Dataset.from_pandas(pd.DataFrame(data=generated_dataset))\n",
    "ds = ds.train_test_split(test_size=1)\n",
    "ds['train'].to_parquet(output_dir / f\"ultrachat_200k_generated/{data_frac}_{frac_len}/train/data.parquet\")\n",
    "ds['test'].to_parquet(output_dir / f\"ultrachat_200k_generated/{data_frac}_{frac_len}/test/data.parquet\")\n",
    "\n",
    "# Delete the llm object and free the memory\n",
    "destroy_model_parallel()\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.distributed.destroy_process_group()\n",
    "print(\"Successfully delete the llm pipeline and free the GPU memory!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
